# ุดุฑูุน ุณุฑุน - ุงุฌุฑุง ูุญู ุฑู All-in-One 2017

## ๐ฏ ุฎูุงุตู ุชูุตูโูุง

### ุณุณุชู ุดูุง:
- **CPU**: Intel i5 ูุณู 7
- **RAM**: 32GB โ
- **GPU**: 4GB VRAM
- **Storage**: 512GB SSD

### ูุฏูโูุง ูพุดููุงุฏ:

#### 1๏ธโฃ ุจุฑุง RL Trading (ูพุฑูฺู ูุนู) โ

**ูุถุนุช**: ูพุฑูฺู ุดูุง ฺฉุงููุงู ุฑู ุณุณุชู ุดูุง ุงุฌุฑุง ูโุดูุฏ!

```bash
# ุชุณุช ุณุฑุน
python test_local_setup.py
```

**ุชูุธูุงุช ุจููู**:
- โ ุงุณุชูุงุฏู ุงุฒ **CPU** ุจู ุฌุง GPU (ุณุฑุนโุชุฑ ุจุฑุง PPO)
- โ ฺฉุงููฺฏ ุจููู: `configs/config_local_optimized.json`
- โ ูุนูุงุฑ ุดุจฺฉู: [256, 256, 128] - ููุงุณุจ

#### 2๏ธโฃ ุจุฑุง LLM (ุงุฎุชุงุฑ - ุชุญูู ุงุญุณุงุณุงุช/ุงุฎุจุงุฑ)

| ูุฏู | ุญุฌู | VRAM | ุณุฑุนุช | ุชูุตู |
|-----|------|------|------|-------|
| **Gemma 2B (Q4)** | 1.6GB | 2GB | 15-20 t/s | โญโญโญโญโญ |
| **Phi-3 Mini** | 2.3GB | 2.5GB | 12-18 t/s | โญโญโญโญ |
| **Llama 3.2 3B** | 2GB | 2GB | 10-15 t/s | โญโญโญ |
| **TinyLlama 1.1B** | <1GB | 1GB | 25+ t/s | โญโญโญ |

---

## ๐ ุฑุงูโุงูุฏุงุฒ ุฏุฑ 3 ูุฑุญูู

### ูุฑุญูู 1: ุชุณุช ุณุณุชู

```bash
# ุงุฌุฑุง ุชุณุช
python test_local_setup.py
```

ุงู ุงุณฺฉุฑูพุช ุจุฑุฑุณ ูโฺฉูุฏ:
- โ Python ู PyTorch
- โ Stable Baselines3
- โ Freqtrade
- โ ููุงุจุน ุณุณุชู
- โ ุชุณุช ุขููุฒุด RL

### ูุฑุญูู 2: ุงุณุชูุงุฏู ุงุฒ ฺฉุงููฺฏ ุจููู

```bash
# ฺฉูพ ฺฉุงููฺฏ ุจููู
cp configs/config_local_optimized.json configs/config.json

# ุง ุงุณุชูุงุฏู ูุณุชูู
freqtrade backtesting \
    --config configs/config_local_optimized.json \
    --strategy MtfScalper_RL_Hybrid \
    --freqaimodel MtfScalperRLModel
```

### ูุฑุญูู 3: ูุตุจ LLM (ุงุฎุชุงุฑ)

```bash
# ูุตุจ Ollama
curl -fsSL https://ollama.com/install.sh | sh

# ุฏุงูููุฏ Gemma 2B
ollama pull gemma2:2b

# ุชุณุช
ollama run gemma2:2b
```

---

## ๐ ุชุบุฑุงุช ฺฉูุฏ ุฏุฑ ฺฉุงููฺฏ ุจููู

```json
{
  "rl_config": {
    "device": "cpu",           // โ ุชุบุฑ ุงุฒ "auto" ุจู "cpu"
    "cpu_count": 4,            // โ ุชุบุฑ ุงุฒ 8 ุจู 4
    "train_cycles": 25,        // โ ฺฉุงูุด ุงุฒ 30 ุจู 25
    "n_steps": 1024,           // โ ฺฉุงูุด ุงุฒ 2048 ุจู 1024
    "batch_size": 64           // โ ุจููู ุจุฑุง CPU
  }
}
```

**ฺุฑุง ุงู ุชุบุฑุงุชุ**
- CPU ุณุฑุนโุชุฑ ุงุฒ GPU 4GB ุจุฑุง ุดุจฺฉูโูุง ฺฉูฺฺฉ PPO
- i5-7th gen ุฏุงุฑุง 4 ฺฉูุฑ ูุฒฺฉ
- ฺฉุงูุด ุญุงูุธู ููุฑุฏ ูุงุฒ
- ุณุฑุนุช ุขููุฒุด ุจูุชุฑ

---

## ๐ก ุงุณุชูุงุฏู ููุฒูุงู RL + LLM

```python
# ูุซุงู: ุชุฑฺฉุจ RL Trading ุจุง ุชุญูู ุงุญุณุงุณุงุช

# 1. RL Training ุฑู CPU
import torch
model = PPO("MlpPolicy", env, device="cpu")

# 2. LLM Inference ุฑู GPU (ููุฒูุงู)
# ุฏุฑ ุชุฑููุงู ุฏฺฏุฑ:
# ollama run gemma2:2b

# 3. Embedding ุจุฑุง Feature Engineering
from sentence_transformers import SentenceTransformer
sentiment = SentenceTransformer('all-MiniLM-L6-v2')  # 80MB
```

ุงู setup ุจูุชุฑู ุงุณุชูุงุฏู ุงุฒ ููุงุจุน ุฑุง ูโฺฉูุฏ:
- **CPU**: RL Training
- **GPU**: LLM Inference
- **RAM**: Data Processing

---

## ๐ง ุนุจโุงุจ

### ูุดฺฉู: ุขููุฒุด RL ุฎู ฺฉูุฏ ุงุณุช

**ุฑุงูโุญู**:
```bash
# 1. ูุทูุฆู ุดูุฏ ุงุฒ CPU ุงุณุชูุงุฏู ูโฺฉูุฏ
grep '"device"' configs/config_local_optimized.json
# ุจุงุฏ "cpu" ุจุงุดุฏ ูู "auto" ุง "cuda"

# 2. ฺฉุงูุด ุชุนุฏุงุฏ cycles
# train_cycles: 25 โ 20

# 3. ฺฉุงูุด ุงูุฏุงุฒู ุดุจฺฉู
# net_arch: [256, 256, 128] โ [128, 128, 64]
```

### ูุดฺฉู: LLM OOM (Out of Memory) ุฑู GPU

**ุฑุงูโุญู**:
```bash
# 1. ุงุณุชูุงุฏู ุงุฒ ูุฏู ฺฉูฺฺฉโุชุฑ
ollama run tinyllama  # ุจู ุฌุง gemma2:2b

# 2. ุง ุงุฌุฑุง ุฑู CPU
CUDA_VISIBLE_DEVICES="" ollama run gemma2:2b
```

### ูุดฺฉู: RAM ฺฉุงู ูุณุช

**ุฑุงูโุญู**:
```bash
# ฺฉุงูุด ุชุนุฏุงุฏ pairโูุง ุฏุฑ whitelist
# pair_whitelist: ["BTC/USDT:USDT", "ETH/USDT:USDT"]  # ููุท 2 ุฌูุช
```

---

## ๐ ุนููฺฉุฑุฏ ููุฑุฏ ุงูุชุธุงุฑ

### RL Training:
- **ุณุฑุนุช**: ~15-20 ุฏููู ุจุฑุง 25 cycles
- **RAM**: 4-6GB ุงุณุชูุงุฏู
- **CPU**: 80-100% ุงุณุชูุงุฏู

### LLM Inference:
- **Gemma 2B**: 15-20 tokens/sec
- **Phi-3 Mini**: 12-18 tokens/sec
- **GPU**: 2-3GB VRAM ุงุณุชูุงุฏู

---

## ๐ ูุงูโูุง ููู

1. **LOCAL_EXECUTION_GUIDE.md** - ุฑุงูููุง ฺฉุงูู ูุงุฑุณ
2. **configs/config_local_optimized.json** - ฺฉุงููฺฏ ุจููู
3. **test_local_setup.py** - ุงุณฺฉุฑูพุช ุชุณุช ุณุณุชู
4. **ุงู ูุงู** - ุดุฑูุน ุณุฑุน

---

## โ ฺฺฉโูุณุช ุขูุงุฏฺฏ

- [ ] Python 3.8+ ูุตุจ ุดุฏู
- [ ] PyTorch ูุตุจ ุดุฏู
- [ ] Stable Baselines3 ูุตุจ ุดุฏู
- [ ] Freqtrade ูุตุจ ุดุฏู
- [ ] test_local_setup.py ุงุฌุฑุง ุดุฏู ู ูููู ุจูุฏู
- [ ] config_local_optimized.json ุจุฑุฑุณ ุดุฏู
- [ ] (ุงุฎุชุงุฑ) Ollama ูุตุจ ุดุฏู

---

## ๐ฏ ูุชุฌูโฺฏุฑ

### โ ูุฏูโูุง ููุงุณุจ ุจุฑุง ุณุณุชู ุดูุง:

**ุจุฑุง RL Trading:**
- โ PPO ุจุง [256, 256, 128] ุฑู CPU
- โ PPO ุจุง [128, 128, 64] ุฑู CPU (ุณุฑุนโุชุฑ)
- โ ูุฏูโูุง ุจุฒุฑฺฏโุชุฑ (ูุงุฒ ุจู GPU ููโุชุฑ)

**ุจุฑุง LLM/NLP:**
- โ Gemma 2B (Q4) - ุจูุชุฑู
- โ Phi-3 Mini (Q4)
- โ Llama 3.2 3B (Q4)
- โ TinyLlama 1.1B
- โ ูุฏูโูุง 7B+ (ูุงุฒ ุจู 8GB+ VRAM)

**ุจุฑุง Embeddings:**
- โ all-MiniLM-L6-v2 (80MB)
- โ paraphrase-MiniLM-L3-v2 (60MB)
- โ distilbert-base-uncased (260MB)

---

## ๐ ููุงุจุน ุงุถุงู

- **Ollama**: https://ollama.com
- **Stable Baselines3**: https://stable-baselines3.readthedocs.io
- **Freqtrade**: https://www.freqtrade.io
- **Sentence Transformers**: https://www.sbert.net

---

**ูููู ุจุงุดุฏ! ๐**
