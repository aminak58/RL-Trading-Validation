{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MtfScalper RL Feature Engineering Analysis\n",
    "## ÿ™ÿ≠ŸÑ€åŸÑ Ÿà ÿ®ÿ±ÿ±ÿ≥€å Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß€å RL ÿ®ÿ±ÿß€å ÿ®Ÿá€åŸÜŸá‚Äåÿ≥ÿßÿ≤€å ÿÆÿ±Ÿàÿ¨\n",
    "\n",
    "ÿß€åŸÜ ŸÜŸàÿ™‚Äåÿ®Ÿà⁄© ÿ®ÿ±ÿß€å:\n",
    "1. ÿ™ÿ≠ŸÑ€åŸÑ ÿßŸáŸÖ€åÿ™ Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß\n",
    "2. ÿ®ÿ±ÿ±ÿ≥€å ŸáŸÖÿ®ÿ≥ÿ™⁄Ø€å ÿ®€åŸÜ features\n",
    "3. ÿßÿ±ÿ≤€åÿßÿ®€å ⁄©€åŸÅ€åÿ™ ÿ≥€å⁄ØŸÜÿßŸÑ‚ÄåŸáÿß€å ÿÆÿ±Ÿàÿ¨\n",
    "4. ÿ™ŸÜÿ∏€åŸÖ Ÿæÿßÿ±ÿßŸÖÿ™ÿ±Ÿáÿß€å reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Freqtrade imports\n",
    "try:\n",
    "    from freqtrade.data.history import load_pair_history\n",
    "    FREQTRADE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FREQTRADE_AVAILABLE = False\n",
    "    print(\"Freqtrade not available, using alternative data loading\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Freqtrade available: {FREQTRADE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PAIR = \"BTC/USDT:USDT\"\n",
    "TIMEFRAME = \"5m\"\n",
    "DATA_PATH = \"user_data/data/binance\"\n",
    "\n",
    "# Try to load data using Freqtrade\n",
    "if FREQTRADE_AVAILABLE:\n",
    "    try:\n",
    "        df = load_pair_history(\n",
    "            datadir=Path(DATA_PATH),\n",
    "            timeframe=TIMEFRAME,\n",
    "            pair=PAIR,\n",
    "            data_format='json',\n",
    "            candle_type='futures'\n",
    "        )\n",
    "        print(f\"Loaded {len(df)} candles for {PAIR} using Freqtrade\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading with Freqtrade: {e}\")\n",
    "        FREQTRADE_AVAILABLE = False\n",
    "\n",
    "# Alternative: Generate sample data if Freqtrade not available\n",
    "if not FREQTRADE_AVAILABLE or df is None:\n",
    "    print(\"Generating sample data for analysis...\")\n",
    "    \n",
    "    # Generate sample price data\n",
    "    np.random.seed(42)\n",
    "    periods = 5000\n",
    "    base_price = 60000\n",
    "    \n",
    "    # Generate realistic price movements\n",
    "    returns = np.random.normal(0.0001, 0.02, periods)\n",
    "    prices = [base_price]\n",
    "    \n",
    "    for ret in returns:\n",
    "        new_price = prices[-1] * (1 + ret)\n",
    "        prices.append(new_price)\n",
    "    \n",
    "    # Create OHLCV data\n",
    "    timestamps = pd.date_range(start='2024-01-01', periods=periods, freq='5T')\n",
    "    \n",
    "    data = {\n",
    "        'open': prices[:-1],\n",
    "        'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices[:-1]],\n",
    "        'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices[:-1]],\n",
    "        'close': prices[1:],\n",
    "        'volume': np.random.lognormal(10, 1, periods)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data, index=timestamps)\n",
    "    print(f\"Generated {len(df)} sample candles\")\n",
    "\n",
    "print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "print(f\"Price range: ${df['close'].min():.0f} - ${df['close'].max():.0f}\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Apply all feature engineering for RL exit optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic indicators\n",
    "    try:\n",
    "        import talib.abstract as ta\n",
    "        TALIB_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        TALIB_AVAILABLE = False\n",
    "        print(\"TA-Lib not available, using manual calculations\")\n",
    "    \n",
    "    if TALIB_AVAILABLE:\n",
    "        # EMAs\n",
    "        df['ema_9'] = ta.EMA(df, timeperiod=9)\n",
    "        df['ema_21'] = ta.EMA(df, timeperiod=21)\n",
    "        df['ema_200'] = ta.EMA(df, timeperiod=200)\n",
    "        \n",
    "        # RSI\n",
    "        df['rsi'] = ta.RSI(df, timeperiod=14)\n",
    "        \n",
    "        # ATR\n",
    "        df['atr'] = ta.ATR(df, timeperiod=14)\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        df['bb_upper'], df['bb_middle'], df['bb_lower'] = ta.BBANDS(df, timeperiod=20)\n",
    "    else:\n",
    "        # Manual calculations\n",
    "        # EMAs\n",
    "        df['ema_9'] = df['close'].ewm(span=9).mean()\n",
    "        df['ema_21'] = df['close'].ewm(span=21).mean()\n",
    "        df['ema_200'] = df['close'].ewm(span=200).mean()\n",
    "        \n",
    "        # RSI\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        df['rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # ATR\n",
    "        high_low = df['high'] - df['low']\n",
    "        high_close = np.abs(df['high'] - df['close'].shift())\n",
    "        low_close = np.abs(df['low'] - df['close'].shift())\n",
    "        true_range = np.maximum(high_low, np.maximum(high_close, low_close))\n",
    "        df['atr'] = true_range.rolling(window=14).mean()\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        df['bb_middle'] = df['close'].rolling(window=20).mean()\n",
    "        df['bb_std'] = df['close'].rolling(window=20).std()\n",
    "        df['bb_upper'] = df['bb_middle'] + (df['bb_std'] * 2)\n",
    "        df['bb_lower'] = df['bb_middle'] - (df['bb_std'] * 2)\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # Exit-Specific Features\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    # Price momentum\n",
    "    df['momentum_5'] = df['close'].pct_change(5)\n",
    "    df['momentum_10'] = df['close'].pct_change(10)\n",
    "    df['momentum_20'] = df['close'].pct_change(20)\n",
    "    \n",
    "    # Acceleration\n",
    "    df['acceleration'] = df['momentum_5'].diff()\n",
    "    \n",
    "    # Distance from recent high/low\n",
    "    df['dist_from_high_20'] = (df['high'].rolling(20).max() - df['close']) / df['close']\n",
    "    df['dist_from_low_20'] = (df['close'] - df['low'].rolling(20).min()) / df['close']\n",
    "    \n",
    "    # Volume patterns\n",
    "    df['volume_ratio_5'] = df['volume'] / df['volume'].rolling(5).mean()\n",
    "    df['volume_ratio_20'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "    \n",
    "    # Spread proxy\n",
    "    df['spread_proxy'] = (df['high'] - df['low']) / df['close']\n",
    "    df['spread_ma_ratio'] = df['spread_proxy'] / df['spread_proxy'].rolling(20).mean()\n",
    "    \n",
    "    # RSI Divergence\n",
    "    price_higher = df['close'] > df['close'].shift(10)\n",
    "    rsi_lower = df['rsi'] < df['rsi'].shift(10)\n",
    "    df['bearish_divergence'] = (price_higher & rsi_lower).astype(int)\n",
    "    \n",
    "    price_lower = df['close'] < df['close'].shift(10)\n",
    "    rsi_higher = df['rsi'] > df['rsi'].shift(10)\n",
    "    df['bullish_divergence'] = (price_lower & rsi_higher).astype(int)\n",
    "    \n",
    "    # Support/Resistance\n",
    "    df['pivot'] = (df['high'] + df['low'] + df['close']) / 3\n",
    "    df['r1'] = 2 * df['pivot'] - df['low']\n",
    "    df['s1'] = 2 * df['pivot'] - df['high']\n",
    "    df['dist_to_r1'] = (df['r1'] - df['close']) / df['close']\n",
    "    df['dist_to_s1'] = (df['close'] - df['s1']) / df['close']\n",
    "    \n",
    "    # Risk score\n",
    "    df['risk_score'] = (\n",
    "        df['spread_proxy'] * 0.3 +\n",
    "        (1 / (df['volume_ratio_5'] + 0.1)) * 0.3 +\n",
    "        df['atr'] / df['close'] * 0.4\n",
    "    )\n",
    "    \n",
    "    return df.fillna(0)\n",
    "\n",
    "# Apply features\n",
    "df = apply_feature_engineering(df)\n",
    "print(f\"Created {len(df.columns)} features\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on correlation with future returns\n",
    "def calculate_feature_importance(df, target_periods=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Calculate feature importance for different prediction horizons\n",
    "    \"\"\"\n",
    "    importance_dict = {}\n",
    "    \n",
    "    feature_cols = [\n",
    "        'momentum_5', 'momentum_10', 'momentum_20', 'acceleration',\n",
    "        'dist_from_high_20', 'dist_from_low_20', 'volume_ratio_5',\n",
    "        'volume_ratio_20', 'spread_proxy', 'bearish_divergence',\n",
    "        'bullish_divergence', 'risk_score', 'rsi', 'atr'\n",
    "    ]\n",
    "    \n",
    "    for period in target_periods:\n",
    "        # Calculate future returns\n",
    "        df[f'future_return_{period}'] = df['close'].shift(-period) / df['close'] - 1\n",
    "        \n",
    "        # Calculate correlations\n",
    "        correlations = {}\n",
    "        for feature in feature_cols:\n",
    "            if feature in df.columns:\n",
    "                corr = df[feature].corr(df[f'future_return_{period}'])\n",
    "                correlations[feature] = abs(corr) if not pd.isna(corr) else 0\n",
    "        \n",
    "        importance_dict[f'{period}_candles'] = correlations\n",
    "    \n",
    "    return pd.DataFrame(importance_dict)\n",
    "\n",
    "importance_df = calculate_feature_importance(df)\n",
    "importance_df = importance_df.sort_values('10_candles', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Results:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Plot importance\n",
    "if len(importance_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    importance_df.head(10).plot(kind='barh', ax=ax)\n",
    "    plt.title('Feature Importance for Exit Timing (Top 10)', fontsize=14)\n",
    "    plt.xlabel('Absolute Correlation with Future Returns')\n",
    "    plt.ylabel('Features')\n",
    "    plt.legend(title='Prediction Horizon')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTop 5 Most Important Features (10 candles):\")\n",
    "    print(importance_df['10_candles'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exit Signal Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_exit_signals(df):\n",
    "    \"\"\"\n",
    "    Analyze potential exit points based on various conditions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define exit conditions\n",
    "    exit_conditions = {\n",
    "        'RSI_Overbought': df['rsi'] > 70,\n",
    "        'RSI_Oversold': df['rsi'] < 30,\n",
    "        'Bearish_Divergence': df['bearish_divergence'] == 1,\n",
    "        'Bullish_Divergence': df['bullish_divergence'] == 1,\n",
    "        'High_Risk_Score': df['risk_score'] > df['risk_score'].quantile(0.8),\n",
    "        'Low_Volume': df['volume_ratio_5'] < 0.5,\n",
    "        'Price_At_Resistance': df['dist_to_r1'] < 0.005,\n",
    "        'Price_At_Support': df['dist_to_s1'] < 0.005,\n",
    "        'High_Momentum': df['momentum_5'] > 0.02,\n",
    "        'Negative_Momentum': df['momentum_5'] < -0.02\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for condition_name, condition in exit_conditions.items():\n",
    "        # Calculate returns after signal\n",
    "        signal_indices = df[condition].index\n",
    "        \n",
    "        if len(signal_indices) > 0:\n",
    "            returns_5 = []\n",
    "            returns_10 = []\n",
    "            \n",
    "            for idx in signal_indices[:-20]:  # Avoid end of dataframe\n",
    "                loc = df.index.get_loc(idx)\n",
    "                if loc + 10 < len(df):\n",
    "                    ret_5 = (df.iloc[loc + 5]['close'] - df.iloc[loc]['close']) / df.iloc[loc]['close']\n",
    "                    ret_10 = (df.iloc[loc + 10]['close'] - df.iloc[loc]['close']) / df.iloc[loc]['close']\n",
    "                    returns_5.append(ret_5)\n",
    "                    returns_10.append(ret_10)\n",
    "            \n",
    "            results[condition_name] = {\n",
    "                'count': len(signal_indices),\n",
    "                'frequency': len(signal_indices) / len(df),\n",
    "                'avg_return_5': np.mean(returns_5) if returns_5 else 0,\n",
    "                'avg_return_10': np.mean(returns_10) if returns_10 else 0,\n",
    "                'win_rate_5': sum(1 for r in returns_5 if r > 0) / len(returns_5) if returns_5 else 0\n",
    "            }\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "exit_analysis = analyze_exit_signals(df)\n",
    "exit_analysis = exit_analysis.sort_values('avg_return_10', ascending=False)\n",
    "\n",
    "print(\"Exit Signal Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(exit_analysis)\n",
    "\n",
    "# Visualize\n",
    "if len(exit_analysis) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Average returns\n",
    "    exit_analysis[['avg_return_5', 'avg_return_10']].plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Average Returns After Exit Signal')\n",
    "    axes[0].set_ylabel('Return (%)')\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Plot 2: Signal frequency vs win rate\n",
    "    axes[1].scatter(exit_analysis['frequency'], exit_analysis['win_rate_5'], s=100)\n",
    "    for idx, row in exit_analysis.iterrows():\n",
    "        axes[1].annotate(idx, (row['frequency'], row['win_rate_5']), fontsize=8)\n",
    "    axes[1].set_xlabel('Signal Frequency')\n",
    "    axes[1].set_ylabel('Win Rate (5 candles)')\n",
    "    axes[1].set_title('Signal Frequency vs Win Rate')\n",
    "    axes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Timeframe Alignment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mtf_alignment(df):\n",
    "    \"\"\"\n",
    "    Analyze the quality of multi-timeframe alignment signals\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate MTF conditions\n",
    "    df['trend_5m'] = (df['ema_9'] > df['ema_21']).astype(int)\n",
    "    \n",
    "    # Resample for higher timeframes\n",
    "    df_15m = df.resample('15T').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    })\n",
    "    \n",
    "    df_1h = df.resample('1H').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Calculate higher TF trends\n",
    "    df_15m['ema_9'] = df_15m['close'].ewm(span=9).mean()\n",
    "    df_15m['ema_21'] = df_15m['close'].ewm(span=21).mean()\n",
    "    df_15m['trend'] = (df_15m['ema_9'] > df_15m['ema_21']).astype(int)\n",
    "    \n",
    "    df_1h['ema_9'] = df_1h['close'].ewm(span=9).mean()\n",
    "    df_1h['ema_21'] = df_1h['close'].ewm(span=21).mean()\n",
    "    df_1h['trend'] = (df_1h['ema_9'] > df_1h['ema_21']).astype(int)\n",
    "    \n",
    "    # Merge trends back\n",
    "    df['trend_15m'] = df_15m['trend'].reindex(df.index, method='ffill')\n",
    "    df['trend_1h'] = df_1h['trend'].reindex(df.index, method='ffill')\n",
    "    \n",
    "    # Calculate alignment\n",
    "    df['mtf_aligned_long'] = (\n",
    "        (df['trend_5m'] == 1) &\n",
    "        (df['trend_15m'] == 1) &\n",
    "        (df['trend_1h'] == 1)\n",
    "    )\n",
    "    \n",
    "    df['mtf_aligned_short'] = (\n",
    "        (df['trend_5m'] == 0) &\n",
    "        (df['trend_15m'] == 0) &\n",
    "        (df['trend_1h'] == 0)\n",
    "    )\n",
    "    \n",
    "    # Analyze performance of aligned signals\n",
    "    aligned_long_signals = df[df['mtf_aligned_long']].index\n",
    "    aligned_short_signals = df[df['mtf_aligned_short']].index\n",
    "    \n",
    "    results = {\n",
    "        'Total_Candles': len(df),\n",
    "        'Aligned_Long_Count': len(aligned_long_signals),\n",
    "        'Aligned_Short_Count': len(aligned_short_signals),\n",
    "        'Aligned_Long_%': len(aligned_long_signals) / len(df) * 100,\n",
    "        'Aligned_Short_%': len(aligned_short_signals) / len(df) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate returns after alignment\n",
    "    if len(aligned_long_signals) > 0:\n",
    "        long_returns = []\n",
    "        for idx in aligned_long_signals[:-20]:\n",
    "            loc = df.index.get_loc(idx)\n",
    "            if loc + 20 < len(df):\n",
    "                ret = (df.iloc[loc + 20]['close'] - df.iloc[loc]['close']) / df.iloc[loc]['close']\n",
    "                long_returns.append(ret)\n",
    "        results['Avg_Long_Return_20'] = np.mean(long_returns) if long_returns else 0\n",
    "    \n",
    "    if len(aligned_short_signals) > 0:\n",
    "        short_returns = []\n",
    "        for idx in aligned_short_signals[:-20]:\n",
    "            loc = df.index.get_loc(idx)\n",
    "            if loc + 20 < len(df):\n",
    "                ret = (df.iloc[loc]['close'] - df.iloc[loc + 20]['close']) / df.iloc[loc]['close']\n",
    "                short_returns.append(ret)\n",
    "        results['Avg_Short_Return_20'] = np.mean(short_returns) if short_returns else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "mtf_results = analyze_mtf_alignment(df)\n",
    "\n",
    "print(\"Multi-Timeframe Alignment Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in mtf_results.items():\n",
    "    if '%' in key or 'Return' in key:\n",
    "        print(f\"{key}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:,}\")\n",
    "\n",
    "# Visualize MTF alignment\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(df.index[-500:], df['close'][-500:], label='Price', alpha=0.7)\n",
    "ax.scatter(df[df['mtf_aligned_long']].index[-50:], df[df['mtf_aligned_long']]['close'][-50:], \n",
    "           color='green', label='Aligned Long', alpha=0.8, marker='^')\n",
    "ax.scatter(df[df['mtf_aligned_short']].index[-50:], df[df['mtf_aligned_short']]['close'][-50:], \n",
    "           color='red', label='Aligned Short', alpha=0.8, marker='v')\n",
    "ax.set_title('MTF Alignment Signals (Last 500 Candles)')\n",
    "ax.set_ylabel('Price')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reward Function Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reward_function(df, weights):\n",
    "    \"\"\"\n",
    "    Simulate different reward function weight combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate trades\n",
    "    trades = []\n",
    "    in_position = False\n",
    "    entry_price = 0\n",
    "    entry_idx = 0\n",
    "    max_profit = 0\n",
    "    \n",
    "    for i in range(100, len(df) - 20):\n",
    "        row = df.iloc[i]\n",
    "        \n",
    "        # Simple entry condition\n",
    "        if not in_position and row['momentum_5'] > 0 and row['rsi'] < 70:\n",
    "            in_position = True\n",
    "            entry_price = row['close']\n",
    "            entry_idx = i\n",
    "            max_profit = 0\n",
    "        \n",
    "        elif in_position:\n",
    "            current_profit = (row['close'] - entry_price) / entry_price\n",
    "            max_profit = max(max_profit, current_profit)\n",
    "            \n",
    "            # Calculate reward components\n",
    "            profit_score = min(10, max(-10, current_profit * 100))\n",
    "            \n",
    "            # Drawdown from max profit\n",
    "            drawdown = max_profit - current_profit if max_profit > 0 else 0\n",
    "            drawdown_score = -5 * drawdown if drawdown > 0.01 else 5\n",
    "            \n",
    "            # Timing score (simplified)\n",
    "            position_duration = i - entry_idx\n",
    "            timing_score = 5 if position_duration < 50 else -position_duration * 0.01\n",
    "            \n",
    "            # Risk/Reward (simplified)\n",
    "            rr_score = 3 if current_profit > 0.01 else -1\n",
    "            \n",
    "            # Calculate total reward\n",
    "            total_reward = (\n",
    "                weights['profit'] * profit_score +\n",
    "                weights['drawdown'] * drawdown_score +\n",
    "                weights['timing'] * timing_score +\n",
    "                weights['risk_reward'] * rr_score\n",
    "            )\n",
    "            \n",
    "            # Exit decision based on reward\n",
    "            if total_reward < -5 or position_duration > 100 or current_profit > 0.03:\n",
    "                trades.append({\n",
    "                    'entry_idx': entry_idx,\n",
    "                    'exit_idx': i,\n",
    "                    'duration': position_duration,\n",
    "                    'profit': current_profit,\n",
    "                    'max_profit': max_profit,\n",
    "                    'drawdown': drawdown,\n",
    "                    'reward': total_reward\n",
    "                })\n",
    "                in_position = False\n",
    "    \n",
    "    return pd.DataFrame(trades)\n",
    "\n",
    "# Test different weight combinations\n",
    "weight_combinations = [\n",
    "    {'profit': 0.35, 'drawdown': 0.25, 'timing': 0.20, 'risk_reward': 0.20},  # Balanced\n",
    "    {'profit': 0.50, 'drawdown': 0.20, 'timing': 0.15, 'risk_reward': 0.15},  # Profit focused\n",
    "    {'profit': 0.25, 'drawdown': 0.40, 'timing': 0.20, 'risk_reward': 0.15},  # Risk focused\n",
    "    {'profit': 0.30, 'drawdown': 0.20, 'timing': 0.35, 'risk_reward': 0.15},  # Timing focused\n",
    "]\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "for i, weights in enumerate(weight_combinations):\n",
    "    trades = simulate_reward_function(df, weights)\n",
    "    \n",
    "    if len(trades) > 0:\n",
    "        results_comparison.append({\n",
    "            'Config': f\"Config_{i+1}\",\n",
    "            'Total_Trades': len(trades),\n",
    "            'Avg_Profit': trades['profit'].mean() * 100,\n",
    "            'Win_Rate': (trades['profit'] > 0).mean() * 100,\n",
    "            'Avg_Duration': trades['duration'].mean(),\n",
    "            'Max_Drawdown': trades['drawdown'].max() * 100,\n",
    "            'Sharpe': trades['profit'].mean() / trades['profit'].std() if trades['profit'].std() > 0 else 0\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(results_comparison)\n",
    "\n",
    "print(\"Reward Function Weight Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "# Visualize comparison\n",
    "if len(comparison_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Metrics to plot\n",
    "    metrics = ['Avg_Profit', 'Win_Rate', 'Avg_Duration', 'Sharpe']\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        if metric in comparison_df.columns:\n",
    "            comparison_df.plot(x='Config', y=metric, kind='bar', ax=ax, legend=False)\n",
    "            ax.set_title(metric.replace('_', ' '))\n",
    "            ax.set_xlabel('')\n",
    "    \n",
    "    plt.suptitle('Reward Function Configuration Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä ÿ™ÿ≠ŸÑ€åŸÑ ŸÜŸáÿß€å€å Ÿà ÿ™Ÿàÿµ€åŸá‚ÄåŸáÿß:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "1. **Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß€å ⁄©ŸÑ€åÿØ€å ÿ®ÿ±ÿß€å ÿÆÿ±Ÿàÿ¨:**\n",
    "   - Momentum indicators (5-20 candles) ÿ®€åÿ¥ÿ™ÿ±€åŸÜ ÿßŸáŸÖ€åÿ™\n",
    "   - Volume patterns ŸÜÿ¥ÿßŸÜ‚ÄåÿØŸáŸÜÿØŸá ⁄©€åŸÅ€åÿ™ ŸÜŸÇÿØ€åŸÜ⁄Ø€å\n",
    "   - Divergence signals ÿ®ÿ±ÿß€å ÿ™ÿ¥ÿÆ€åÿµ reversal\n",
    "\n",
    "2. **ÿ™ŸÜÿ∏€åŸÖÿßÿ™ Reward Function:**\n",
    "   - Ÿàÿ≤ŸÜ Profit: 35% (ÿ™ÿπÿßÿØŸÑ ÿ®€åŸÜ ÿ≥ŸàÿØ Ÿà ÿ±€åÿ≥⁄©)\n",
    "   - Ÿàÿ≤ŸÜ Drawdown Control: 25% (⁄©ŸÜÿ™ÿ±ŸÑ ÿ±€åÿ≥⁄©)\n",
    "   - Ÿàÿ≤ŸÜ Timing: 20% (ÿÆÿ±Ÿàÿ¨ ÿØÿ± ÿ≤ŸÖÿßŸÜ ŸÖŸÜÿßÿ≥ÿ®)\n",
    "   - Ÿàÿ≤ŸÜ Risk/Reward: 20% (⁄©€åŸÅ€åÿ™ ŸÖÿπÿßŸÖŸÑŸá)\n",
    "\n",
    "3. **ÿ®Ÿá€åŸÜŸá‚Äåÿ≥ÿßÿ≤€å‚ÄåŸáÿß€å Ÿæ€åÿ¥ŸÜŸáÿßÿØ€å:**\n",
    "   - ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ Ensemble Models ÿ®ÿ±ÿß€å predictions\n",
    "   - ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ Market Regime Detection\n",
    "   - Ÿæ€åÿßÿØŸá‚Äåÿ≥ÿßÿ≤€å Adaptive Position Sizing\n",
    "\n",
    "4. **Risk Management:**\n",
    "   - Max position duration: 300 candles (25 hours)\n",
    "   - Emergency exit at -3% loss\n",
    "   - Breakeven trigger at +2% profit\n",
    "\n",
    "5. **ŸÖÿ±ÿßÿ≠ŸÑ ÿ®ÿπÿØ€å:**\n",
    "   - Phase 2: ÿ¢ŸÖŸàÿ≤ÿ¥ ŸÖÿØŸÑ RL ÿ®ÿß ÿØÿßÿØŸá‚ÄåŸáÿß€å 18 ŸÖÿßŸá\n",
    "   - Phase 3: Backtesting Ÿà walk-forward analysis\n",
    "   - Phase 4: Paper trading ÿ®ÿ±ÿß€å 2-4 ŸáŸÅÿ™Ÿá\n",
    "   - Phase 5: Live deployment ÿ®ÿß position sizing ŸÖÿ≠ÿØŸàÿØ\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "# Save analysis results\n",
    "analysis_results = {\n",
    "    'data_info': {\n",
    "        'total_candles': len(df),\n",
    "        'date_range': f\"{df.index[0]} to {df.index[-1]}\",\n",
    "        'price_range': f\"${df['close'].min():.0f} - ${df['close'].max():.0f}\"\n",
    "    },\n",
    "    'feature_count': len(df.columns),\n",
    "    'best_exit_signals': exit_analysis.head(3).to_dict() if len(exit_analysis) > 0 else {},\n",
    "    'mtf_alignment': mtf_results,\n",
    "    'best_reward_config': comparison_df.iloc[0].to_dict() if len(comparison_df) > 0 else {}\n",
    "}\n",
    "\n",
    "import json\n",
    "try:\n",
    "    with open('user_data/notebooks/feature_analysis_results.json', 'w') as f:\n",
    "        json.dump(analysis_results, f, indent=2, default=str)\n",
    "    print(\"\\n‚úÖ Results saved to feature_analysis_results.json\")\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è Could not save results to file\")\n",
    "\n",
    "print(\"\\nüéØ Analysis Complete! Ready for RL model optimization.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
