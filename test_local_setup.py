#!/usr/bin/env python3
"""
ØªØ³Øª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø­Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… All-in-One 2017
i5-7th gen / 32GB RAM / 4GB GPU
"""

import sys
import subprocess
import platform

def print_header(text):
    """Ú†Ø§Ù¾ Ù‡Ø¯Ø± Ø²ÛŒØ¨Ø§"""
    print("\n" + "="*60)
    print(f"  {text}")
    print("="*60)

def check_python():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø³Ø®Ù‡ Python"""
    print_header("Ø¨Ø±Ø±Ø³ÛŒ Python")
    version = sys.version_info
    print(f"âœ“ Python {version.major}.{version.minor}.{version.micro}")
    if version.major < 3 or (version.major == 3 and version.minor < 8):
        print("âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Python 3.8+")
        return False
    print("âœ… Ù†Ø³Ø®Ù‡ Python Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª")
    return True

def check_pytorch():
    """Ø¨Ø±Ø±Ø³ÛŒ PyTorch Ùˆ CUDA"""
    print_header("Ø¨Ø±Ø±Ø³ÛŒ PyTorch")
    try:
        import torch
        print(f"âœ“ PyTorch {torch.__version__}")

        # Ø¨Ø±Ø±Ø³ÛŒ CUDA
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            print(f"âœ“ CUDA available: {torch.version.cuda}")
            print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
            print(f"âœ“ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            print("âš  CUDA not available - will use CPU")

        # Ø¨Ø±Ø±Ø³ÛŒ MPS (Ø¨Ø±Ø§ÛŒ Mac)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("âœ“ Apple Silicon MPS available")

        print("âœ… PyTorch Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø§Ø³Øª")
        return True
    except ImportError:
        print("âŒ PyTorch Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡")
        return False

def check_stable_baselines3():
    """Ø¨Ø±Ø±Ø³ÛŒ Stable Baselines3"""
    print_header("Ø¨Ø±Ø±Ø³ÛŒ Stable Baselines3")
    try:
        import stable_baselines3 as sb3
        print(f"âœ“ Stable Baselines3 {sb3.__version__}")

        # ØªØ³Øª Ø³Ø§Ø®Øª ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡
        from stable_baselines3 import PPO
        import gym

        env = gym.make("CartPole-v1")
        model = PPO("MlpPolicy", env, verbose=0, device="cpu")
        print("âœ“ PPO model test successful")

        print("âœ… Stable Baselines3 Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯")
        return True
    except ImportError:
        print("âŒ Stable Baselines3 Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡")
        return False
    except Exception as e:
        print(f"âš  Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª: {e}")
        return False

def check_freqtrade():
    """Ø¨Ø±Ø±Ø³ÛŒ Freqtrade"""
    print_header("Ø¨Ø±Ø±Ø³ÛŒ Freqtrade")
    try:
        import freqtrade
        print(f"âœ“ Freqtrade Ù†ØµØ¨ Ø´Ø¯Ù‡")

        # Ø¨Ø±Ø±Ø³ÛŒ FreqAI
        try:
            from freqtrade.freqai.data_kitchen import FreqaiDataKitchen
            print("âœ“ FreqAI available")
        except:
            print("âš  FreqAI may not be available")

        print("âœ… Freqtrade Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø§Ø³Øª")
        return True
    except ImportError:
        print("âŒ Freqtrade Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡")
        return False

def check_system_resources():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…"""
    print_header("Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…")

    # CPU
    import psutil
    cpu_count = psutil.cpu_count()
    print(f"âœ“ CPU Cores: {cpu_count}")

    # RAM
    ram = psutil.virtual_memory()
    ram_gb = ram.total / (1024**3)
    print(f"âœ“ Total RAM: {ram_gb:.1f} GB")
    print(f"âœ“ Available RAM: {ram.available / (1024**3):.1f} GB")

    if ram_gb < 16:
        print("âš  Ú©Ù…ØªØ± Ø§Ø² 16GB RAM - Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯")
    else:
        print("âœ… RAM Ú©Ø§ÙÛŒ Ø§Ø³Øª")

    # Disk
    disk = psutil.disk_usage('/')
    disk_gb = disk.free / (1024**3)
    print(f"âœ“ Free Disk: {disk_gb:.1f} GB")

    return True

def test_small_rl_training():
    """ØªØ³Øª Ø¢Ù…ÙˆØ²Ø´ RL Ú©ÙˆÚ†Ú©"""
    print_header("ØªØ³Øª Ø¢Ù…ÙˆØ²Ø´ RL")

    try:
        import gym
        from stable_baselines3 import PPO
        import time

        print("Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ· CartPole...")
        env = gym.make("CartPole-v1")

        # ØªØ³Øª CPU
        print("\nØªØ³Øª Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ CPU...")
        start_time = time.time()
        model_cpu = PPO(
            "MlpPolicy",
            env,
            verbose=0,
            device="cpu",
            n_steps=128,
            batch_size=32
        )
        model_cpu.learn(total_timesteps=1000)
        cpu_time = time.time() - start_time
        print(f"âœ“ Ø¢Ù…ÙˆØ²Ø´ CPU: {cpu_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

        # ØªØ³Øª GPU (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
        try:
            import torch
            if torch.cuda.is_available():
                print("\nØªØ³Øª Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ GPU...")
                start_time = time.time()
                model_gpu = PPO(
                    "MlpPolicy",
                    env,
                    verbose=0,
                    device="cuda",
                    n_steps=128,
                    batch_size=32
                )
                model_gpu.learn(total_timesteps=1000)
                gpu_time = time.time() - start_time
                print(f"âœ“ Ø¢Ù…ÙˆØ²Ø´ GPU: {gpu_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

                if cpu_time < gpu_time:
                    print("\nğŸ’¡ ØªÙˆØµÛŒÙ‡: Ø¨Ø±Ø§ÛŒ PPO Ú©ÙˆÚ†Ú©ØŒ CPU Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø³Øª!")
                else:
                    print(f"\nğŸ’¡ GPU {cpu_time/gpu_time:.1f}x Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø³Øª")
        except:
            pass

        print("\nâœ… ØªØ³Øª RL Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯")
        return True

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª RL: {e}")
        return False

def check_ollama():
    """Ø¨Ø±Ø±Ø³ÛŒ Ollama Ø¨Ø±Ø§ÛŒ LLM"""
    print_header("Ø¨Ø±Ø±Ø³ÛŒ Ollama (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)")

    try:
        result = subprocess.run(
            ["ollama", "--version"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            print(f"âœ“ Ollama: {result.stdout.strip()}")

            # Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù†ØµØ¨ Ø´Ø¯Ù‡
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                timeout=5
            )
            if "gemma2:2b" in result.stdout:
                print("âœ“ Gemma 2B Ù†ØµØ¨ Ø´Ø¯Ù‡")
            else:
                print("âš  Gemma 2B Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ - Ø¨Ø±Ø§ÛŒ Ù†ØµØ¨:")
                print("  ollama pull gemma2:2b")

            print("âœ… Ollama Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª")
            return True
        else:
            print("âš  Ollama Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡")
            print("Ø¨Ø±Ø§ÛŒ Ù†ØµØ¨:")
            print("  curl -fsSL https://ollama.com/install.sh | sh")
            return False
    except FileNotFoundError:
        print("âš  Ollama Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡")
        print("Ø¨Ø±Ø§ÛŒ Ù†ØµØ¨:")
        print("  curl -fsSL https://ollama.com/install.sh | sh")
        return False
    except Exception as e:
        print(f"âš  Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ollama: {e}")
        return False

def print_recommendations():
    """Ú†Ø§Ù¾ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ"""
    print_header("ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ")

    print("""
ğŸ“Š Ø¨Ø±Ø§ÛŒ RL Trading (Ù¾Ø±ÙˆÚ˜Ù‡ ÙØ¹Ù„ÛŒ):
   âœ“ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CPU Ø¨Ø±Ø§ÛŒ PPO
   âœ“ cpu_count: 4 Ø¯Ø± config
   âœ“ device: "cpu"

ğŸ¤– Ø¨Ø±Ø§ÛŒ LLM (Ø§Ø®ØªÛŒØ§Ø±ÛŒ):
   âœ“ Gemma 2B (1.6GB) - Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨
   âœ“ Phi-3 Mini (2.3GB) - Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø®ÙˆØ¨
   âœ“ TinyLlama (900MB) - Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ†

ğŸ’¾ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹:
   CPU: RL Training
   GPU: LLM Inference
   RAM: Data Processing

ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…:
   âœ“ LOCAL_EXECUTION_GUIDE.md - Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„
   âœ“ configs/config_local_optimized.json - Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ù‡ÛŒÙ†Ù‡
    """)

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    print_header("ØªØ³Øª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø­Ù„ÛŒ - All-in-One 2017")
    print("i5-7th gen / 32GB RAM / 4GB GPU")

    results = []

    # Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
    results.append(("Python", check_python()))
    results.append(("PyTorch", check_pytorch()))
    results.append(("Stable Baselines3", check_stable_baselines3()))
    results.append(("Freqtrade", check_freqtrade()))
    results.append(("System Resources", check_system_resources()))

    # ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø®ØªÛŒØ§Ø±ÛŒ
    results.append(("RL Training Test", test_small_rl_training()))
    results.append(("Ollama", check_ollama()))

    # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
    print_header("Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬")

    passed = sum(1 for _, status in results if status)
    total = len(results)

    for name, status in results:
        status_text = "âœ… OK" if status else "âŒ FAIL"
        print(f"{name:25s} {status_text}")

    print(f"\nÙ†ØªÛŒØ¬Ù‡: {passed}/{total} ØªØ³Øª Ù…ÙˆÙÙ‚")

    if passed >= 5:  # Ø­Ø¯Ø§Ù‚Ù„ 5 ØªØ³Øª Ø¨Ø§ÛŒØ¯ Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´Ø¯
        print("\nğŸ‰ Ø³ÛŒØ³ØªÙ… Ø´Ù…Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!")
        print_recommendations()
    else:
        print("\nâš  Ø¨Ø±Ø®ÛŒ Ù…Ø´Ú©Ù„Ø§Øª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ - Ù„Ø·ÙØ§Ù‹ requirements.txt Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯:")
        print("  pip install -r requirements.txt")

    return passed >= 5

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nÙ…ØªÙˆÙ‚Ù Ø´Ø¯ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")
        sys.exit(1)
